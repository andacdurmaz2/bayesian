{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAeeGac6IeoVHhF5bozDx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andacdurmaz2/bayesian/blob/Amirreza/MCMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fmc2nsqofsFP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.stats import invwishart"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def b_draw(beta, sigma_b, sigma_e, y, X):\n",
        "    \"\"\"\n",
        "    Draws a sample for each pixel-level random effect b_i.\n",
        "\n",
        "    This function implements the full conditional distribution:\n",
        "    b_i | beta, sigma_e, sigma_b, y_i ~ MVN(mu_bi, sigma_bi)\n",
        "\n",
        "    This is based on the model:\n",
        "    y_i ~ N(X_i @ b_i, sigma_e * I)\n",
        "    b_i ~ N(beta, sigma_b)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    beta : np.array\n",
        "        The (z,) or (z, 1) population-level mean vector.\n",
        "    sigma_b : np.array\n",
        "        The (z, z) prior covariance matrix for b_i, (Sigma_b).\n",
        "    sigma_e : float\n",
        "        The scalar error variance (sigma^2_epsilon).\n",
        "    y : list of np.array\n",
        "        A list of n arrays. Each element y[i] is the (m_i, 1) or (m_i,)\n",
        "        observation vector for group i.\n",
        "    X : list of np.array\n",
        "        A list of n arrays. Each element X[i] is the (m_i, z) design\n",
        "        matrix for group i.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of n numpy arrays, where each element is a (z,)\n",
        "        draw for b_i.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the number of groups\n",
        "    n = len(y)\n",
        "\n",
        "    # Get the dimension z from the shape of sigma_b\n",
        "    z = sigma_b.shape[0]\n",
        "\n",
        "    # --- Pre-compute Inverses and check dimensions ---\n",
        "    # We add a small jitter for numerical stability\n",
        "    jitter = 1e-6 * np.eye(z)\n",
        "\n",
        "    try:\n",
        "        sigma_b_inv = np.linalg.inv(sigma_b)\n",
        "    except np.linalg.LinAlgError:\n",
        "        sigma_b_inv = np.linalg.inv(sigma_b + jitter)\n",
        "\n",
        "    # Ensure beta is a column vector (z, 1) for matrix math\n",
        "    if beta.ndim == 1:\n",
        "        beta = beta.reshape(-1, 1)\n",
        "\n",
        "    # List to store the samples\n",
        "    b_samples_list = []\n",
        "\n",
        "    # --- Loop over all n groups ---\n",
        "    for i in range(n):\n",
        "        X_i = X[i]\n",
        "        y_i = y[i]\n",
        "\n",
        "        # Ensure y_i is a column vector (m_i, 1)\n",
        "        if y_i.ndim == 1:\n",
        "            y_i = y_i.reshape(-1, 1)\n",
        "\n",
        "        # --- 1. Calculate Posterior Covariance (Sigma_bi) ---\n",
        "\n",
        "        # X_i' * X_i\n",
        "        XtX = X_i.T @ X_i\n",
        "\n",
        "        # Posterior precision: Lambda = (1/sigma_e)*X'X + sigma_b_inv\n",
        "        lambda_bi = (1.0 / sigma_e) * XtX + sigma_b_inv\n",
        "\n",
        "        # Posterior covariance: Sigma_bi = Lambda^{-1}\n",
        "        try:\n",
        "            sigma_bi = np.linalg.inv(lambda_bi)\n",
        "        except np.linalg.LinAlgError:\n",
        "            sigma_bi = np.linalg.inv(lambda_bi + jitter)\n",
        "\n",
        "        # --- 2. Calculate Posterior Mean (mu_bi) ---\n",
        "\n",
        "        # X_i' * y_i\n",
        "        Xty = X_i.T @ y_i\n",
        "\n",
        "        # K = (1/sigma_e)*X'y + sigma_b_inv @ beta\n",
        "        K_i = (1.0 / sigma_e) * Xty + (sigma_b_inv @ beta)\n",
        "\n",
        "        # mu_bi = Sigma_bi @ K\n",
        "        mu_bi = sigma_bi @ K_i\n",
        "\n",
        "        # --- 3. Draw the sample ---\n",
        "        # np.random.multivariate_normal requires the mean to be a 1D array\n",
        "        b_i_sample = np.random.multivariate_normal(mu_bi.flatten(), sigma_bi)\n",
        "\n",
        "        b_samples_list.append(b_i_sample)\n",
        "\n",
        "    return b_samples_list"
      ],
      "metadata": {
        "id": "EdSrklcDfwyY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beta_draw(sigma_b, sigma_e, y, c, n, X):\n",
        "    \"\"\"\n",
        "    Draws a single sample for the population coefficients beta.\n",
        "\n",
        "    This function implements the full conditional distribution:\n",
        "    beta | sigma_e, sigma_b, y ~ MVN(mu_beta, sigma_beta)\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma_b : np.array\n",
        "        The (z, z) prior covariance matrix for beta_i, (Sigma_b).\n",
        "    sigma_e : float\n",
        "        The scalar error variance (sigma^2_epsilon).\n",
        "    y : list of np.array\n",
        "        A list of n arrays. Each element y[i] is the (m_i, 1) observation\n",
        "        vector for group i.\n",
        "    c : float\n",
        "        A scalar hyperparameter for the prior precision of beta.\n",
        "    n : int\n",
        "        The number of groups (the length of X and y).\n",
        "    X : list of np.array\n",
        "        A list of n arrays. Each element X[i] is the (m_i, z) design\n",
        "        matrix for group i.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.array\n",
        "        A (z,) numpy array representing a single draw from the posterior.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the dimension z from the shape of sigma_b\n",
        "    z = sigma_b.shape[0]\n",
        "\n",
        "    # --- Pre-compute Inverses and Identity ---\n",
        "    # We add a small jitter for numerical stability in case\n",
        "    # sigma_b is near-singular.\n",
        "    jitter = 1e-6 * np.eye(z)\n",
        "\n",
        "    try:\n",
        "        sigma_b_inv = np.linalg.inv(sigma_b)\n",
        "    except np.linalg.LinAlgError:\n",
        "        sigma_b_inv = np.linalg.inv(sigma_b + jitter)\n",
        "\n",
        "    I_z = np.eye(z)\n",
        "\n",
        "    # --- Initialize Summation Terms ---\n",
        "\n",
        "    # This is the large summation term in the precision matrix (Lambda)\n",
        "    # sum( sigma_b_inv @ ( (1/sigma_e)*X_i'X_i + sigma_b_inv )^{-1} @ sigma_b_inv )\n",
        "    sum_precision_term = np.zeros((z, z))\n",
        "\n",
        "    # This is the large summation term for the mean (K = Lambda @ mu)\n",
        "    # sum( sigma_b_inv @ ( (1/sigma_e)*X_i'X_i + sigma_b_inv )^{-1} @ (1/sigma_e)*X_i'y_i )\n",
        "    sum_mean_term_K = np.zeros((z, 1))\n",
        "\n",
        "    # --- Loop over all n groups ---\n",
        "    for i in range(n):\n",
        "        X_i = X[i]\n",
        "        y_i = y[i]\n",
        "\n",
        "        # Ensure y_i is a column vector (m_i, 1)\n",
        "        if y_i.ndim == 1:\n",
        "            y_i = y_i.reshape(-1, 1)\n",
        "\n",
        "        # --- Calculate intermediate terms ---\n",
        "        # X_i' * X_i\n",
        "        XtX = X_i.T @ X_i\n",
        "        # X_i' * y_i\n",
        "        Xty = X_i.T @ y_i\n",
        "\n",
        "        # ( (1/sigma_e) * X_i'X_i + sigma_b_inv )\n",
        "        inner_term = (1.0 / sigma_e) * XtX + sigma_b_inv\n",
        "\n",
        "        # ( (1/sigma_e) * X_i'X_i + sigma_b_inv )^{-1}\n",
        "        try:\n",
        "            inner_inv = np.linalg.inv(inner_term)\n",
        "        except np.linalg.LinAlgError:\n",
        "            inner_inv = np.linalg.inv(inner_term + jitter)\n",
        "\n",
        "        # --- Add to the Precision Sum ---\n",
        "        # term_i = sigma_b_inv @ inner_inv @ sigma_b_inv\n",
        "        sum_precision_term += sigma_b_inv @ inner_inv @ sigma_b_inv\n",
        "\n",
        "        # --- Add to the Mean Sum (K) ---\n",
        "        # term_i = sigma_b_inv @ inner_inv @ ( (1/sigma_e) * X_i'y_i )\n",
        "        sum_mean_term_K += sigma_b_inv @ inner_inv @ ( (1.0 / sigma_e) * Xty )\n",
        "\n",
        "    # --- 1. Calculate the Posterior Precision Matrix (Lambda) ---\n",
        "    # Lambda = n*Sigma_b_inv + (1/c)*I - sum_precision_term\n",
        "    lambda_beta = (n * sigma_b_inv) + ((1.0 / c) * I_z) - sum_precision_term\n",
        "\n",
        "    # --- 2. Calculate the Posterior Covariance Matrix (Sigma) ---\n",
        "    # Sigma = Lambda^{-1}\n",
        "    try:\n",
        "        sigma_beta = np.linalg.inv(lambda_beta)\n",
        "    except np.linalg.LinAlgError:\n",
        "        sigma_beta = np.linalg.inv(lambda_beta + jitter)\n",
        "\n",
        "    # --- 3. Calculate the Posterior Mean (mu) ---\n",
        "    # mu = Sigma @ K\n",
        "    mu_beta = sigma_beta @ sum_mean_term_K\n",
        "\n",
        "    # --- 4. Draw the sample from MVN(mu, Sigma) ---\n",
        "    # np.random.multivariate_normal requires the mean to be a 1D array\n",
        "    beta_sample = np.random.multivariate_normal(mu_beta.flatten(), sigma_beta)\n",
        "\n",
        "    return beta_sample\n"
      ],
      "metadata": {
        "id": "pR1G_2defzUu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigma_e_draw(beta, sigma_b, y, n, X, b, d, c, m):\n",
        "    \"\"\"\n",
        "    Draws a single sample for the residual variance sigma_e^2.\n",
        "\n",
        "    This function implements the full conditional distribution:\n",
        "    sigma_e^2 | beta, b, Sigma_b, y ~ IG(c_e_posterior, d_e_posterior)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    beta : np.array\n",
        "        Population coefficients (unused in this step, but part of state).\n",
        "    sigma_b : np.array\n",
        "        Covariance matrix for b_i (unused in this step, but part of state).\n",
        "    y : list of np.array\n",
        "        A list of n arrays. Each element y[i] is the (m_i,) or (m_i, 1)\n",
        "        observation vector for group i.\n",
        "    n : int\n",
        "        The number of groups (the length of X, y, and b).\n",
        "    X : list of np.array\n",
        "        A list of n arrays. Each element X[i] is the (m_i, z) design\n",
        "        matrix for group i.\n",
        "    b : list of np.array\n",
        "        A list of n arrays. Each element b[i] is the (z,) or (z, 1)\n",
        "        current coefficient vector for group i.\n",
        "    d : float\n",
        "        The prior scale parameter (d_epsilon) for the Inverse-Gamma.\n",
        "    c : float\n",
        "        The prior shape parameter (c_epsilon) for the Inverse-Gamma.\n",
        "    m : int\n",
        "        The total number of observations (sum of all m_i across all n groups).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        A single draw for sigma_e^2 from its posterior distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate posterior shape parameter\n",
        "    # c_e_post = c_e + m/2\n",
        "    c_posterior = c + m / 2.0\n",
        "\n",
        "    # 2. Calculate the total Sum of Squared Errors (SSE)\n",
        "    # SSE = sum_{i=1}^n (y_i - X_i*b_i)' * (y_i - X_i*b_i)\n",
        "    total_sse = 0.0\n",
        "    for i in range(n):\n",
        "        y_i = y[i].flatten()  # Ensure 1D (m_i,)\n",
        "        X_i = X[i]            # (m_i, z)\n",
        "        b_i = b[i].flatten()  # Ensure 1D (z,)\n",
        "\n",
        "        # Calculate residuals: r_i = y_i - X_i @ b_i\n",
        "        residuals = y_i - (X_i @ b_i)\n",
        "\n",
        "        # Add squared errors for group i: r_i' @ r_i\n",
        "        # Using np.dot for 1D arrays is equivalent to inner product\n",
        "        total_sse += np.dot(residuals, residuals)\n",
        "\n",
        "    # 3. Calculate posterior scale parameter\n",
        "    # d_e_post = d_e + (1/2) * SSE\n",
        "    d_posterior = d + 0.5 * total_sse\n",
        "\n",
        "    # 4. Draw from the posterior Inverse-Gamma(c_posterior, d_posterior)\n",
        "    #\n",
        "    # We can sample from an Inverse-Gamma(a, b) (shape, scale)\n",
        "    # by sampling from a Gamma(a, 1/b) (shape, rate)\n",
        "    # and taking the reciprocal.\n",
        "    # numpy.random.gamma(shape, scale) uses scale = 1/rate.\n",
        "\n",
        "    gamma_sample = np.random.gamma(shape=c_posterior, scale=1.0 / d_posterior)\n",
        "\n",
        "    # The sample from IG is the reciprocal of the Gamma sample\n",
        "    sigma_e_squared_sample = 1.0 / gamma_sample\n",
        "\n",
        "    return sigma_e_squared_sample"
      ],
      "metadata": {
        "id": "3GDvZIHpf1f4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigma_b_draw(beta, b, y, n, etha_b, S):\n",
        "    \"\"\"\n",
        "    Draws a single sample for the random-effect covariance matrix Sigma_b.\n",
        "\n",
        "    This function implements the full conditional distribution:\n",
        "    Sigma_b | beta, b, y ~ IW(eta_posterior, S_posterior)\n",
        "\n",
        "    It assumes the common conjugate prior IW(etha_b, S_0), where\n",
        "    the prior scale matrix S_0 is parameterized as S_0 = etha_b * S.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    beta : np.array\n",
        "        The (z,) or (z, 1) population-level mean vector.\n",
        "    b : list of np.array\n",
        "        A list of n arrays. Each element b[i] is the (z,) or (z, 1)\n",
        "        current coefficient vector for group i.\n",
        "    y : list (unused)\n",
        "        The observation data (conditionally independent given b).\n",
        "    n : int\n",
        "        The number of groups (the length of b).\n",
        "    etha_b : float\n",
        "        The prior degrees of freedom (eta_b).\n",
        "    S : np.array\n",
        "        The (z, z) prior *mean* matrix. The prior scale matrix\n",
        "        is calculated as S_0 = etha_b * S.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.array\n",
        "        A (z, z) matrix drawn from the posterior Inverse-Wishart.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the dimension z from the shape of beta\n",
        "    z = beta.shape[0]\n",
        "\n",
        "    # --- 1. Calculate Posterior Degrees of Freedom ---\n",
        "    # eta_posterior = etha_b + n\n",
        "    eta_posterior = etha_b + n\n",
        "\n",
        "    # --- 2. Calculate Posterior Scale Matrix ---\n",
        "\n",
        "    # Start with the prior scale matrix: S_0 = etha_b * S\n",
        "    S_0 = etha_b * S\n",
        "\n",
        "    # Ensure beta is a column vector (z, 1) for calculations\n",
        "    if beta.ndim == 1:\n",
        "        beta_col = beta.reshape(-1, 1)\n",
        "    else:\n",
        "        beta_col = beta\n",
        "\n",
        "    # Calculate the sum of squared deviations\n",
        "    sum_of_squares = np.zeros((z, z))\n",
        "\n",
        "    for i in range(n):\n",
        "        b_i = b[i]\n",
        "\n",
        "        # Ensure b_i is a column vector (z, 1)\n",
        "        if b_i.ndim == 1:\n",
        "            b_i_col = b_i.reshape(-1, 1)\n",
        "        else:\n",
        "            b_i_col = b_i\n",
        "\n",
        "        # Calculate deviation: (b_i - beta)\n",
        "        deviation = b_i_col - beta_col\n",
        "\n",
        "        # Calculate outer product: (b_i - beta)(b_i - beta)'\n",
        "        # This is (z, 1) @ (1, z) -> (z, z)\n",
        "        sum_of_squares += deviation @ deviation.T\n",
        "\n",
        "    # The posterior scale matrix: S_posterior = S_0 + sum_of_squares\n",
        "    S_posterior = S_0 + sum_of_squares\n",
        "\n",
        "    # --- 3. Draw from the posterior Inverse-Wishart ---\n",
        "\n",
        "    # Add a small jitter for numerical stability, as the\n",
        "    # scale matrix must be positive definite.\n",
        "    jitter = 1e-6 * np.eye(z)\n",
        "\n",
        "    # Use scipy.stats.invwishart(df, scale)\n",
        "    try:\n",
        "        sigma_b_sample = invwishart.rvs(df=eta_posterior, scale=S_posterior)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # If it fails (e.g., not positive definite), add jitter\n",
        "        sigma_b_sample = invwishart.rvs(df=eta_posterior, scale=S_posterior + jitter)\n",
        "\n",
        "    return sigma_b_sample"
      ],
      "metadata": {
        "id": "iQEUmlxMf3R1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_one_gibbs(y, X, sigma_b, sigma_e, b, beta, priors):\n",
        "    \"\"\"\n",
        "    Runs one full iteration of the Gibbs sampler.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : list\n",
        "        List of n (m_i,) observation vectors.\n",
        "    X : list\n",
        "        List of n (m_i, z) design matrices.\n",
        "    sigma_b : np.array\n",
        "        Current state of the (z, z) random-effect covariance.\n",
        "    sigma_e : float\n",
        "        Current state of the residual variance.\n",
        "    b : list\n",
        "        List of n (z,) current random-effect vectors.\n",
        "    beta : np.array\n",
        "        Current state of the (z,) population-effect vector.\n",
        "    priors : dict\n",
        "        A dictionary containing all hyperparameters:\n",
        "        - 'c_beta': Prior variance scalar for beta.\n",
        "        - 'c_epsilon': Prior shape for sigma_e.\n",
        "        - 'd_epsilon': Prior scale for sigma_e.\n",
        "        - 'eta_b': Prior degrees of freedom for sigma_b.\n",
        "        - 'S_b': Prior mean matrix (z, z) for sigma_b.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple of the updated parameters:\n",
        "        (sigma_b_new, sigma_e_new, b_new, beta_new)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get constants from data\n",
        "    n = len(y)\n",
        "    m = sum(len(y_i) for y_i in y)\n",
        "\n",
        "    # 1. Update population coefficients beta\n",
        "    beta_new = beta_draw(\n",
        "        sigma_b=sigma_b,\n",
        "        sigma_e=sigma_e,\n",
        "        y=y,\n",
        "        c=priors['c_beta'],\n",
        "        n=n,\n",
        "        X=X\n",
        "    )\n",
        "\n",
        "    # 2. Update pixel-level random effects b_i\n",
        "    #    Uses the NEWLY sampled beta\n",
        "    b_new = b_draw(\n",
        "        beta=beta_new,\n",
        "        sigma_b=sigma_b,\n",
        "        sigma_e=sigma_e,\n",
        "        y=y,\n",
        "        X=X\n",
        "    )\n",
        "\n",
        "    # 3. Update residual variance sigma_e\n",
        "    #    Uses the NEWLY sampled b_new\n",
        "    sigma_e_new = sigma_e_draw(\n",
        "        beta=beta_new,  # (unused, but for signature)\n",
        "        sigma_b=sigma_b, # (unused, but for signature)\n",
        "        y=y,\n",
        "        n=n,\n",
        "        X=X,\n",
        "        b=b_new,\n",
        "        d=priors['d_epsilon'],\n",
        "        c=priors['c_epsilon'],\n",
        "        m=m\n",
        "    )\n",
        "\n",
        "    # 4. Update random-effect covariance sigma_b\n",
        "    #    Uses the NEWLY sampled beta_new and b_new\n",
        "    sigma_b_new = sigma_b_draw(\n",
        "        beta=beta_new,\n",
        "        b=b_new,\n",
        "        y=y, # (unused, but for signature)\n",
        "        n=n,\n",
        "        etha_b=priors['eta_b'],\n",
        "        S=priors['S_b']\n",
        "    )\n",
        "\n",
        "    # Return the new state\n",
        "    return sigma_b_new, sigma_e_new, b_new, beta_new\n"
      ],
      "metadata": {
        "id": "Jxb8iUe_f5KR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mcmc(y_list, X_list, priors, n_iter=2000, n_burn=1000):\n",
        "    \"\"\"\n",
        "    Runs the full MCMC chain.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get dimensions\n",
        "    n = len(y_list)\n",
        "    z = X_list[0].shape[1]\n",
        "\n",
        "    # --- Initialize the chain ---\n",
        "    print(\"Initializing chain...\")\n",
        "    # Simple initial values\n",
        "    sigma_e_curr = 1.0\n",
        "    sigma_b_curr = np.eye(z)\n",
        "    beta_curr = np.zeros(z)\n",
        "\n",
        "    # Initialize b_i's to zero\n",
        "    b_curr = [np.zeros(z) for _ in range(n)]\n",
        "\n",
        "    # --- Storage for samples ---\n",
        "    n_samples = n_iter - n_burn\n",
        "    if n_samples <= 0:\n",
        "        raise ValueError(\"n_iter must be greater than n_burn\")\n",
        "\n",
        "    beta_samples = np.zeros((n_samples, z))\n",
        "    sigma_e_samples = np.zeros(n_samples)\n",
        "    sigma_b_samples = np.zeros((n_samples, z, z))\n",
        "    # We'll just store samples for the first group's b_i\n",
        "    b_0_samples = np.zeros((n_samples, z))\n",
        "\n",
        "    print(f\"Running MCMC for {n_iter} iterations (burn-in: {n_burn})...\")\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Run one iteration\n",
        "        sigma_b_curr, sigma_e_curr, b_curr, beta_curr = run_one_gibbs(\n",
        "            y_list, X_list,\n",
        "            sigma_b_curr, sigma_e_curr, b_curr, beta_curr,\n",
        "            priors\n",
        "        )\n",
        "\n",
        "        # Store samples after burn-in\n",
        "        if i >= n_burn:\n",
        "            idx = i - n_burn\n",
        "            beta_samples[idx, :] = beta_curr\n",
        "            sigma_e_samples[idx] = sigma_e_curr\n",
        "            sigma_b_samples[idx, :, :] = sigma_b_curr\n",
        "            b_0_samples[idx, :] = b_curr[0]\n",
        "\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"Iteration {i+1}/{n_iter}...\")\n",
        "\n",
        "    print(\"MCMC finished.\")\n",
        "\n",
        "    return {\n",
        "        'beta': beta_samples,\n",
        "        'sigma_e': sigma_e_samples,\n",
        "        'sigma_b': sigma_b_samples,\n",
        "        'b_0': b_0_samples\n",
        "    }"
      ],
      "metadata": {
        "id": "1L-grtrpf67M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- 1. Simulate Data ---\n",
        "    np.random.seed(42)\n",
        "\n",
        "    n_groups = 50\n",
        "    m_obs_per_group = 30\n",
        "    z_features = 3\n",
        "\n",
        "    # True parameters\n",
        "    true_beta = np.array([5.0, -2.0, 1.0])\n",
        "    true_sigma_b = np.array([\n",
        "        [1.0, 0.5, 0.0],\n",
        "        [0.5, 1.2, 0.3],\n",
        "        [0.0, 0.3, 0.8]\n",
        "    ])\n",
        "    true_sigma_e = 1.5\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    true_b_list = []\n",
        "\n",
        "    print(\"Simulating data...\")\n",
        "    for _ in range(n_groups):\n",
        "        # Draw group-level params\n",
        "        b_i = np.random.multivariate_normal(true_beta, true_sigma_b)\n",
        "        true_b_list.append(b_i)\n",
        "\n",
        "        # Create design matrix X_i\n",
        "        X_i = np.ones((m_obs_per_group, z_features))\n",
        "        X_i[:, 1:] = np.random.randn(m_obs_per_group, z_features - 1)\n",
        "\n",
        "        # Create observations y_i\n",
        "        error = np.random.normal(0, np.sqrt(true_sigma_e), m_obs_per_group)\n",
        "        y_i = X_i @ b_i + error\n",
        "\n",
        "        X_list.append(X_i)\n",
        "        y_list.append(y_i)\n",
        "\n",
        "    print(f\"Data simulated: n={n_groups}, m_i={m_obs_per_group}, z={z_features}\")\n",
        "\n",
        "    # --- 2. Define Priors ---\n",
        "    priors = {\n",
        "        'c_beta': 100.0,            # Vague prior for beta\n",
        "        'c_epsilon': 0.01,          # Vague prior for sigma_e\n",
        "        'd_epsilon': 0.01,          # Vague prior for sigma_e\n",
        "        'eta_b': z_features + 2,    # Prior df for sigma_b (min for defined mean)\n",
        "        'S_b': np.eye(z_features)   # Prior mean matrix for sigma_b\n",
        "    }\n",
        "\n",
        "    # --- 3. Run MCMC ---\n",
        "    samples = run_mcmc(y_list, X_list, priors, n_iter=3000, n_burn=1500)\n",
        "\n",
        "    # --- 4. Show Results ---\n",
        "    print(\"\\n--- Posterior Means vs. True Values ---\")\n",
        "\n",
        "    print(\"\\nBeta (Population Coefficients):\")\n",
        "    print(f\"  Posterior Mean: {np.mean(samples['beta'], axis=0)}\")\n",
        "    print(f\"  True Value:     {true_beta}\")\n",
        "\n",
        "    print(\"\\nsigma_e (Residual Variance):\")\n",
        "    print(f\"  Posterior Mean: {np.mean(samples['sigma_e']):.4f}\")\n",
        "    print(f\"  True Value:     {true_sigma_e:.4f}\")\n",
        "\n",
        "    print(\"\\nsigma_b (Random-Effect Covariance):\")\n",
        "    print(f\"  Posterior Mean:\\n{np.mean(samples['sigma_b'], axis=0)}\")\n",
        "    print(f\"  True Value:\\n{true_sigma_b}\")\n",
        "\n",
        "    print(\"\\nb_0 (Random Effects for Group 0):\")\n",
        "    print(f\"  Posterior Mean: {np.mean(samples['b_0'], axis=0)}\")\n",
        "    print(f\"  True Value:     {true_b_list[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4NIXZRZf9fP",
        "outputId": "cc4b1096-6028-4219-cb75-d922f9b7582a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating data...\n",
            "Data simulated: n=50, m_i=30, z=3\n",
            "Initializing chain...\n",
            "Running MCMC for 3000 iterations (burn-in: 1500)...\n",
            "Iteration 500/3000...\n",
            "Iteration 1000/3000...\n",
            "Iteration 1500/3000...\n",
            "Iteration 2000/3000...\n",
            "Iteration 2500/3000...\n",
            "Iteration 3000/3000...\n",
            "MCMC finished.\n",
            "\n",
            "--- Posterior Means vs. True Values ---\n",
            "\n",
            "Beta (Population Coefficients):\n",
            "  Posterior Mean: [ 4.77378963 -2.25464008  1.06023217]\n",
            "  True Value:     [ 5. -2.  1.]\n",
            "\n",
            "sigma_e (Residual Variance):\n",
            "  Posterior Mean: 1.4961\n",
            "  True Value:     1.5000\n",
            "\n",
            "sigma_b (Random-Effect Covariance):\n",
            "  Posterior Mean:\n",
            "[[ 1.06170375  0.57574189 -0.1371528 ]\n",
            " [ 0.57574189  1.22873807  0.14551469]\n",
            " [-0.1371528   0.14551469  0.91781636]]\n",
            "  True Value:\n",
            "[[1.  0.5 0. ]\n",
            " [0.5 1.2 0.3]\n",
            " [0.  0.3 0.8]]\n",
            "\n",
            "b_0 (Random Effects for Group 0):\n",
            "  Posterior Mean: [ 4.91827541 -2.86818844  1.46277361]\n",
            "  True Value:     [ 4.80785565 -2.74676893  1.17264313]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGyufj-Ef_cV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}